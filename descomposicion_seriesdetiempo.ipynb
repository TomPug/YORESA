{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ad45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Crear capa de puntos a partir de las coordenadas ETRS89\n",
    "# Crear geometr√≠a de puntos\n",
    "df = pd.read_excel(r\"E:\\SAR_UVa\\Coord_IFN_Extremadura.xlsx\")\n",
    "geometry = [Point(xy) for xy in zip(df['XETRS89_H30'], df['YETRS89_H30'])]\n",
    "\n",
    "# Crear GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:25830')\n",
    "\n",
    "print(\"Capa de puntos creada exitosamente\")\n",
    "print(f\"N√∫mero de puntos: {len(gdf)}\")\n",
    "print(f\"CRS: {gdf.crs}\")\n",
    "print(f\"\\nPrimeros 5 puntos:\")\n",
    "print(gdf.head())\n",
    "gdf.to_file(r\"E:\\SAR_UVa\\shps\\puntos_ifn_rioja.geojson\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh = pd.read_csv(r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_modificado.csv\")\n",
    "print(df_coh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "\n",
    "input_path = r\"E:\\SAR_UVa\\coherence_stack.nc\"\n",
    "\n",
    "# M√©todo 1: Ver con rioxarray\n",
    "raster = rxr.open_rasterio(input_path)\n",
    "print(\"Coordenadas:\", raster.coords)\n",
    "print(\"Atributos:\", raster.attrs)\n",
    "\n",
    "# M√©todo 2: Ver con rasterio\n",
    "with rasterio.open(input_path) as src:\n",
    "    print(\"\\nN√∫mero de bandas:\", src.count)\n",
    "    print(\"\\nDescripciones de bandas:\")\n",
    "    for i in range(min(5, src.count)):  # Primeras 5 bandas\n",
    "        print(f\"  Banda {i+1}: {src.descriptions[i]}\")\n",
    "    \n",
    "    print(\"\\nTags de la primera banda:\")\n",
    "    print(src.tags(1))\n",
    "    \n",
    "    print(\"\\nMetadata global:\")\n",
    "    print(src.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d083dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de serie temporal por ID\n",
    "# Ajusta el ID y, si aplica, la banda\n",
    "id_parcela = 23\n",
    "band_seleccionada = None  # ej.: 'VV' o 'VH'\n",
    "\n",
    "# Asegurar fecha en formato datetime\n",
    "if 'fecha' in df_coh.columns:\n",
    "    df_coh['fecha'] = pd.to_datetime(df_coh['fecha'])\n",
    "\n",
    "filtro = df_coh['ID_PARCELA'] == id_parcela\n",
    "if band_seleccionada is not None and 'band' in df_coh.columns:\n",
    "    filtro &= df_coh['band'] == band_seleccionada\n",
    "\n",
    "serie = (\n",
    "    df_coh.loc[filtro]\n",
    "    .sort_values('fecha')\n",
    "    .set_index('fecha')['coherencia']\n",
    ")\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(serie.index, serie.values, linewidth=1)\n",
    "ax.set_title(f\"Serie temporal de coherencia - ID {id_parcela}\" + (f\" - {band_seleccionada}\" if band_seleccionada else \"\"))\n",
    "ax.set_xlabel(\"Fecha\")\n",
    "ax.set_ylabel(\"Coherencia\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Ticks mensuales\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dec3c8",
   "metadata": {},
   "source": [
    "## Codigo de calculo de parametros estadisticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d62561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script para calcular estad√≠sticas de series temporales con AUC del a√±o medio\n",
    "Incluye:\n",
    "- √Årea bajo la curva (AUC) del a√±o medio por banda\n",
    "- Media y desviaci√≥n est√°ndar por estaci√≥n\n",
    "- ACF en lags espec√≠ficos\n",
    "- Q-test (Ljung-Box)\n",
    "- √çndices del periodograma\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy import integrate\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "# Ruta del archivo CSV de entrada\n",
    "input_file = r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_modificado.csv\"\n",
    "output_file = 'estadisticas_series_temporales_.csv'\n",
    "\n",
    "# Par√°metros de an√°lisis\n",
    "fs = 1.0      # 1 observaci√≥n por semana\n",
    "nperseg = 52  # ventana anual\n",
    "freq_muestreo = 52  # frecuencia de muestreo anual (semanas)\n",
    "\n",
    "# Lags para ACF\n",
    "acf_lags = [26, 52, 78, 104, 156]\n",
    "\n",
    "# Lags para Q-test (Ljung-Box)\n",
    "qtest_lags = [1, 2, 3, 26, 52, 104, 156]\n",
    "\n",
    "# ============================================================================\n",
    "# CARGA DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE SERIES TEMPORALES CON AUC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not Path(input_file).exists():\n",
    "    print(f\"‚ùå Error: El archivo {input_file} no existe\")\n",
    "    exit(1)\n",
    "\n",
    "band_weekly = pd.read_csv(input_file)\n",
    "band_weekly['fecha'] = pd.to_datetime(band_weekly['fecha'])\n",
    "\n",
    "print(f\"\\n‚úÖ Datos cargados desde: {input_file}\")\n",
    "print(f\"   Registros: {len(band_weekly)}\")\n",
    "print(f\"   Columnas: {band_weekly.columns.tolist()}\")\n",
    "\n",
    "# Obtener bandas disponibles\n",
    "bands = band_weekly['band'].unique()\n",
    "print(f\"   Bandas encontradas: {bands}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "results = []\n",
    "total_parcelas = len(band_weekly['ID_PARCELA'].unique())\n",
    "contador = 0\n",
    "\n",
    "for fid in band_weekly['ID_PARCELA'].unique():\n",
    "    contador += 1\n",
    "    if contador % 50 == 0:\n",
    "        print(f\"Procesando parcela {contador}/{total_parcelas}...\")\n",
    "    \n",
    "    # Obtener la especie principal para este FID\n",
    "    sp_ppal = band_weekly[band_weekly['ID_PARCELA'] == fid]['SP_PPAL'].iloc[0]\n",
    "    \n",
    "    # Iterar por cada banda\n",
    "    for band in bands:\n",
    "        \n",
    "        # Filtrar datos por FID y banda\n",
    "        data_fid_band = band_weekly[\n",
    "            (band_weekly['ID_PARCELA'] == fid) &\n",
    "            (band_weekly['band'] == band)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(data_fid_band) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Serie temporal completa\n",
    "        ts = (\n",
    "            data_fid_band\n",
    "            .groupby('fecha')['coherencia']\n",
    "            .mean()\n",
    "            .dropna()\n",
    "            .sort_index()\n",
    "        )\n",
    "        \n",
    "        if len(ts) < nperseg:\n",
    "            continue\n",
    "        \n",
    "        # Diccionario para almacenar resultados de este FID-banda\n",
    "        row_data = {\n",
    "            'ID_PARCELA': fid,\n",
    "            'SP_PPAL': sp_ppal,\n",
    "            'band': band\n",
    "        }\n",
    "        \n",
    "        # ========== 0. √ÅREA BAJO LA CURVA DEL A√ëO MEDIO POR BANDA ==========\n",
    "        # Obtener el a√±o medio (agregaci√≥n semanal por semana del a√±o)\n",
    "        data_fid_band['week_of_year'] = data_fid_band['fecha'].dt.isocalendar().week\n",
    "        \n",
    "        # Agrupar por semana del a√±o y calcular la media\n",
    "        yearly_mean = (\n",
    "            data_fid_band\n",
    "            .groupby('week_of_year')['coherencia']\n",
    "            .mean()\n",
    "            .sort_index()\n",
    "        )\n",
    "        \n",
    "        if len(yearly_mean) > 0:\n",
    "            # Calcular AUC usando integraci√≥n trapezoidal\n",
    "            x = np.arange(len(yearly_mean))  # semanas 0-51\n",
    "            y = yearly_mean.values\n",
    "            \n",
    "            # AUC con m√©todo trapezoidal\n",
    "            auc = integrate.trapezoid(y, x)\n",
    "            row_data['auc_yearly_mean'] = auc\n",
    "            \n",
    "            # AUC normalizado (dividido por n√∫mero de semanas)\n",
    "            row_data['auc_yearly_mean_normalized'] = auc / len(yearly_mean)\n",
    "            \n",
    "            # Valor promedio del a√±o (alternativa simple)\n",
    "            row_data['mean_yearly_value'] = np.mean(y)\n",
    "            \n",
    "            # M√≠nimo y m√°ximo del a√±o medio\n",
    "            row_data['min_yearly_value'] = np.min(y)\n",
    "            row_data['max_yearly_value'] = np.max(y)\n",
    "            row_data['std_yearly_value'] = np.std(y)\n",
    "        else:\n",
    "            row_data['auc_yearly_mean'] = np.nan\n",
    "            row_data['auc_yearly_mean_normalized'] = np.nan\n",
    "            row_data['mean_yearly_value'] = np.nan\n",
    "            row_data['min_yearly_value'] = np.nan\n",
    "            row_data['max_yearly_value'] = np.nan\n",
    "            row_data['std_yearly_value'] = np.nan\n",
    "        \n",
    "        # ========== 1. MEDIA Y DESVIACI√ìN EST√ÅNDAR POR ESTACI√ìN ==========\n",
    "        data_fid_band['season'] = data_fid_band['fecha'].dt.quarter.map({\n",
    "            1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'\n",
    "        })\n",
    "        \n",
    "        for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
    "            season_data = data_fid_band[data_fid_band['season'] == season]['coherencia']\n",
    "            row_data[f'mean_{season}'] = season_data.mean() if len(season_data) > 0 else np.nan\n",
    "            row_data[f'std_{season}'] = season_data.std() if len(season_data) > 0 else np.nan\n",
    "        \n",
    "        # ========== 2. ACF EN LAGS ESPEC√çFICOS ==========\n",
    "        try:\n",
    "            # Calcular ACF\n",
    "            acf_values = acf(ts.values, nlags=max(acf_lags), fft=True)\n",
    "            \n",
    "            for lag in acf_lags:\n",
    "                if lag < len(acf_values):\n",
    "                    row_data[f'acf_lag_{lag}'] = acf_values[lag]\n",
    "                else:\n",
    "                    row_data[f'acf_lag_{lag}'] = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error ACF para ID_PARCELA={fid}, band={band}: {e}\")\n",
    "            for lag in acf_lags:\n",
    "                row_data[f'acf_lag_{lag}'] = np.nan\n",
    "        \n",
    "        # ========== 3. Q-TEST (LJUNG-BOX) EN LAGS ESPEC√çFICOS ==========\n",
    "        try:\n",
    "            # Calcular Ljung-Box test\n",
    "            lb_result = acorr_ljungbox(ts.values, lags=qtest_lags, return_df=True)\n",
    "            \n",
    "            for lag in qtest_lags:\n",
    "                if lag in lb_result.index:\n",
    "                    row_data[f'qtest_stat_{lag}'] = lb_result.loc[lag, 'lb_stat']\n",
    "                    row_data[f'qtest_pvalue_{lag}'] = lb_result.loc[lag, 'lb_pvalue']\n",
    "                else:\n",
    "                    row_data[f'qtest_stat_{lag}'] = np.nan\n",
    "                    row_data[f'qtest_pvalue_{lag}'] = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error Q-test para ID_PARCELA={fid}, band={band}: {e}\")\n",
    "            for lag in qtest_lags:\n",
    "                row_data[f'qtest_stat_{lag}'] = np.nan\n",
    "                row_data[f'qtest_pvalue_{lag}'] = np.nan\n",
    "        \n",
    "        # ========== 4. √çNDICES DEL PERIODOGRAMA ==========\n",
    "        try:\n",
    "            # Calcular periodograma con Welch\n",
    "            freqs, power = welch(\n",
    "                ts.values, \n",
    "                fs=fs, \n",
    "                detrend='constant',\n",
    "                scaling='density'\n",
    "            )\n",
    "            \n",
    "            # Convertir frecuencias a periodos\n",
    "            mask = freqs > 0\n",
    "            periodos = 1.0 / freqs[mask]\n",
    "            power = power[mask]\n",
    "            \n",
    "            # Ciclos de inter√©s\n",
    "            ciclos = np.array([freq_muestreo/3, freq_muestreo/2, freq_muestreo])\n",
    "            \n",
    "            # Encontrar posiciones de los ciclos\n",
    "            pos_ciclos = [np.argmin(np.abs(periodos - ciclo)) for ciclo in ciclos]\n",
    "            \n",
    "            # Extraer bandas de los ciclos\n",
    "            bands_power = power[pos_ciclos]\n",
    "            \n",
    "            # 4.1 Seasonality Mode (periodo con m√°xima potencia)\n",
    "            max_idx = np.argmax(bands_power)\n",
    "            row_data['seasonality_mode'] = ciclos[max_idx]\n",
    "            \n",
    "            # 4.2 Fisher Kappa (max / mean)\n",
    "            max_power = np.max(bands_power)\n",
    "            mean_power = np.mean(bands_power)\n",
    "            row_data['fisher_kappa'] = max_power / mean_power if mean_power > 0 else np.nan\n",
    "            \n",
    "            # 4.3 Seasonality Stability\n",
    "            power_ciclo_anual_adelante = np.sum(power[pos_ciclos[2]:])\n",
    "            row_data['seasonality_stability'] = (\n",
    "                max_power / power_ciclo_anual_adelante \n",
    "                if power_ciclo_anual_adelante > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # 4.4 Plurianual Cycles\n",
    "            power_plurianual = np.sum(power[:pos_ciclos[2]])\n",
    "            power_total = np.sum(power)\n",
    "            row_data['plurianual_cycles'] = (\n",
    "                power_plurianual / power_total \n",
    "                if power_total > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # 4.5 Seasonality Amplitude\n",
    "            row_data['seasonality_amplitude'] = max_power\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error Periodograma para fid={fid}, band={band}: {e}\")\n",
    "            row_data['seasonality_mode'] = np.nan\n",
    "            row_data['fisher_kappa'] = np.nan\n",
    "            row_data['seasonality_stability'] = np.nan\n",
    "            row_data['plurianual_cycles'] = np.nan\n",
    "            row_data['seasonality_amplitude'] = np.nan\n",
    "        \n",
    "        # Agregar fila a resultados\n",
    "        results.append(row_data)\n",
    "\n",
    "# ============================================================================\n",
    "# GUARDAR RESULTADOS\n",
    "# ============================================================================\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Reordenar columnas\n",
    "cols = ['ID_PARCELA', 'SP_PPAL', 'band'] + [col for col in df_results.columns if col not in ['ID_PARCELA', 'SP_PPAL', 'band']]\n",
    "df_results = df_results[cols]\n",
    "\n",
    "# Guardar CSV\n",
    "df_results.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ AN√ÅLISIS COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Archivo guardado en: {output_file}\")\n",
    "print(f\"Total de registros: {len(df_results)}\")\n",
    "print(f\"\\nColumnas generadas ({len(df_results.columns)}):\")\n",
    "for i, col in enumerate(df_results.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nüìä Resumen de AUC por banda:\")\n",
    "auc_summary = df_results.groupby('band')[['auc_yearly_mean', 'auc_yearly_mean_normalized', 'mean_yearly_value']].describe()\n",
    "print(auc_summary)\n",
    "\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdf1831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34393389289008225"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4790000/13927095\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer el CSV\n",
    "df_coh = pd.read_csv(r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura.csv\")\n",
    "\n",
    "print(\"Columnas originales:\", df_coh.columns.tolist())\n",
    "print(\"Forma:\", df_coh.shape)\n",
    "\n",
    "# Identificar columnas de variables (todas excepto las de ID)\n",
    "id_cols = ['Unnamed: 0', 'ID', 'ID_PARCELA']\n",
    "var_cols = [col for col in df_coh.columns if col not in id_cols]\n",
    "\n",
    "print(f\"\\nN√∫mero de columnas de variables: {len(var_cols)}\")\n",
    "\n",
    "# Generar rango de fechas\n",
    "# Asumiendo frecuencia semanal desde una fecha inicial\n",
    "fecha_inicio = pd.Timestamp('2017-03-01')\n",
    "fechas = pd.date_range(start=fecha_inicio, periods=len(var_cols), freq='6D')  # Semanal\n",
    "# Si es diario, cambiar 'W' por 'D'\n",
    "# Si es mensual, cambiar 'W' por 'MS'\n",
    "\n",
    "# Renombrar columnas\n",
    "rename_dict = {col: fecha.strftime('%Y-%m-%d') for col, fecha in zip(var_cols, fechas)}\n",
    "df_coh_renamed = df_coh.rename(columns=rename_dict)\n",
    "\n",
    "# Mantener solo ID_PARCELA y las columnas de fechas (eliminar Unnamed y ID si prefieres)\n",
    "df_coh_renamed = df_coh_renamed[['ID_PARCELA'] + list(rename_dict.values())]\n",
    "\n",
    "print(\"\\nPrimeras columnas renombradas:\", df_coh_renamed.columns[:5].tolist())\n",
    "print(df_coh_renamed.head())\n",
    "\n",
    "# Guardar\n",
    "df_coh_renamed.to_csv(r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_con_fechas.csv\", index=False)\n",
    "print(\"\\n‚úÖ Archivo guardado: valores_coherencia_extremadura_con_fechas.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar el archivo con fechas\n",
    "df_coh_renamed = pd.read_csv(r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_con_fechas.csv\")\n",
    "\n",
    "print(\"Forma del df original:\", df_coh_renamed.shape)\n",
    "print(\"Primeras columnas:\", df_coh_renamed.columns[:5].tolist())\n",
    "\n",
    "# Separar ID_PARCELA de las fechas\n",
    "id_col = df_coh_renamed['ID_PARCELA']\n",
    "fecha_cols = [col for col in df_coh_renamed.columns if col != 'ID_PARCELA']\n",
    "\n",
    "# Convertir fechas a datetime\n",
    "fechas = pd.to_datetime(fecha_cols)\n",
    "\n",
    "# ========== PAR√ÅMETROS MSTL (M√öLTIPLES ESTACIONALIDADES) ==========\n",
    "seasonal_periods = [52]  # semanas (aprox. 6 meses, 1 a√±o, 2 a√±os)\n",
    "# ================================================================\n",
    "\n",
    "resultados_mstl = []\n",
    "total_ids = len(df_coh_renamed)\n",
    "min_required = 2 * max(seasonal_periods) + 1\n",
    "\n",
    "for idx, row in df_coh_renamed.iterrows():\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Procesando {idx + 1}/{total_ids}...\")\n",
    "    \n",
    "    id_parcela = row['ID_PARCELA']\n",
    "    valores = row[fecha_cols].values\n",
    "    ts = pd.Series(valores, index=fechas)\n",
    "    \n",
    "    # Remuestrear semanalmente\n",
    "    ts_resampleado = ts.resample('W').mean()\n",
    "    \n",
    "    # MSTL no admite NaN: eliminar faltantes\n",
    "    ts_resampleado = ts_resampleado.dropna()\n",
    "    \n",
    "    if len(ts_resampleado) > min_required:\n",
    "        try:\n",
    "            mstl = MSTL(ts_resampleado, periods=seasonal_periods)\n",
    "            result = mstl.fit()\n",
    "            \n",
    "            trend = result.trend.values\n",
    "            residual = result.resid.values\n",
    "            seasonal_df = result.seasonal\n",
    "            original = ts_resampleado.values\n",
    "            fechas_resampleadas = ts_resampleado.index\n",
    "            \n",
    "            # Asegurar nombres consistentes para estacionales\n",
    "            if isinstance(seasonal_df, pd.Series):\n",
    "                seasonal_df = seasonal_df.to_frame()\n",
    "            if len(seasonal_df.columns) == len(seasonal_periods):\n",
    "                seasonal_df = seasonal_df.copy()\n",
    "                seasonal_df.columns = [f\"seasonal_{p}\" for p in seasonal_periods]\n",
    "            \n",
    "            for i, fecha in enumerate(fechas_resampleadas):\n",
    "                row_out = {\n",
    "                    'ID_PARCELA': id_parcela,\n",
    "                    'fecha': fecha,\n",
    "                    'original': original[i],\n",
    "                    'trend': trend[i],\n",
    "                    'residual': residual[i]\n",
    "                }\n",
    "                for p in seasonal_periods:\n",
    "                    col_name = f\"seasonal_{p}\"\n",
    "                    if col_name in seasonal_df.columns:\n",
    "                        row_out[col_name] = seasonal_df.iloc[i][col_name]\n",
    "                    else:\n",
    "                        # Fallback por √≠ndice si los nombres no coinciden\n",
    "                        col_idx = seasonal_periods.index(p)\n",
    "                        row_out[col_name] = seasonal_df.iloc[i].iloc[col_idx]\n",
    "                resultados_mstl.append(row_out)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error MSTL para ID {id_parcela}: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  ID {id_parcela}: insuficientes datos ({len(ts_resampleado)} puntos, m√≠nimo requerido: {min_required})\")\n",
    "\n",
    "df_mstl = pd.DataFrame(resultados_mstl)\n",
    "\n",
    "print(\"\\n‚úÖ Descomposici√≥n MSTL completada\")\n",
    "print(f\"Per√≠odos estacionales utilizados: {seasonal_periods}\")\n",
    "print(f\"Total de registros: {len(df_mstl)}\")\n",
    "print(\"\\nEstructura del resultado:\")\n",
    "print(df_mstl.head(10))\n",
    "print(\"\\nEstad√≠sticas:\")\n",
    "print(df_mstl[['original', 'trend', 'residual'] + [f'seasonal_{p}' for p in seasonal_periods]].describe())\n",
    "\n",
    "# Guardar resultado\n",
    "df_mstl.to_csv(r\"E:\\SAR_UVa\\CSV'S\\descomposicion_mstl.csv\", index=False)\n",
    "print(\"\\n‚úÖ Archivo guardado: descomposicion_mstl.csv\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Cargar datos MSTL\n",
    "df_mstl = pd.read_csv(r\"E:\\SAR_UVa\\CSV'S\\descomposicion_mstl.csv\")\n",
    "df_mstl['fecha'] = pd.to_datetime(df_mstl['fecha'])\n",
    "\n",
    "# ========== CAMBIAR ESTE VALOR PARA VISUALIZAR OTRA PARCELA ==========\n",
    "id_parcela_seleccionada = 20\n",
    "# ===================================================================\n",
    "for id in df_mstl['ID_PARCELA'].unique()[:10]:\n",
    "    id_parcela_seleccionada = id\n",
    "    datos_parcela = df_mstl[df_mstl['ID_PARCELA'] == id_parcela_seleccionada].copy()\n",
    "    datos_parcela = datos_parcela.sort_values('fecha')\n",
    "\n",
    "    if len(datos_parcela) == 0:\n",
    "        print(f\"‚ö†Ô∏è  No hay datos para ID_PARCELA = {id_parcela_seleccionada}\")\n",
    "        print(f\"IDs disponibles: {sorted(df_mstl['ID_PARCELA'].unique())}\")\n",
    "    else:\n",
    "        n_season = len(seasonal_periods)\n",
    "        nrows = n_season + 3  # original, trend, estacionales, residual\n",
    "        fig, axes = plt.subplots(nrows, 1, figsize=(14, 2.5 * nrows))\n",
    "        \n",
    "        # Plot 1: Original\n",
    "        axes[0].plot(datos_parcela['fecha'], datos_parcela['original'], 'b-', linewidth=1.5, label='Original')\n",
    "        axes[0].set_ylabel('Original', fontsize=10)\n",
    "        axes[0].set_title(f'MSTL - ID Parcela {id_parcela_seleccionada}', fontsize=12, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(loc='upper right')\n",
    "        \n",
    "        # Plot 2: Trend\n",
    "        axes[1].plot(datos_parcela['fecha'], datos_parcela['trend'], 'g-', linewidth=1.5, label='Trend')\n",
    "        axes[1].set_ylabel('Trend', fontsize=10)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(loc='upper right')\n",
    "        \n",
    "        # Plots estacionales\n",
    "        for i, p in enumerate(seasonal_periods):\n",
    "            col = f'seasonal_{p}'\n",
    "            axes[2 + i].plot(datos_parcela['fecha'], datos_parcela[col], linewidth=1.5, label=col)\n",
    "            axes[2 + i].set_ylabel(col, fontsize=10)\n",
    "            axes[2 + i].grid(True, alpha=0.3)\n",
    "            axes[2 + i].legend(loc='upper right')\n",
    "        \n",
    "        # Plot Residual\n",
    "        axes[-1].plot(datos_parcela['fecha'], datos_parcela['residual'], 'purple', linewidth=1, label='Residual')\n",
    "        axes[-1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "        axes[-1].set_xlabel('Fecha', fontsize=10)\n",
    "        axes[-1].set_ylabel('Residual', fontsize=10)\n",
    "        axes[-1].grid(True, alpha=0.3)\n",
    "        axes[-1].legend(loc='upper right')\n",
    "        \n",
    "        for ax in axes:\n",
    "            ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Estad√≠sticas para ID_PARCELA = {id_parcela_seleccionada}\")\n",
    "        print(f\"N√∫mero de observaciones: {len(datos_parcela)}\")\n",
    "        print(f\"Rango de fechas: {datos_parcela['fecha'].min()} a {datos_parcela['fecha'].max()}\")\n",
    "        print(\"\\nValores promedio:\")\n",
    "        print(f\"  Original: {datos_parcela['original'].mean():.4f}\")\n",
    "        print(f\"  Trend: {datos_parcela['trend'].mean():.4f}\")\n",
    "        for p in seasonal_periods:\n",
    "            print(f\"  seasonal_{p}: {datos_parcela[f'seasonal_{p}'].mean():.4f}\")\n",
    "        print(f\"  Residual: {datos_parcela['residual'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Usar resultados MSTL\n",
    "df_mstl = pd.read_csv(r\"E:\\SAR_UVa\\CSV'S\\descomposicion_mstl.csv\")\n",
    "df_mstl['fecha'] = pd.to_datetime(df_mstl['fecha'])\n",
    "\n",
    "# Frecuencia de muestreo: 1 observaci√≥n por semana\n",
    "fs = 1.0\n",
    "\n",
    "# Componentes estacionales disponibles\n",
    "seasonal_cols = [col for col in df_mstl.columns if col.startswith('seasonal_')]\n",
    "\n",
    "# Iterar por componente estacional\n",
    "for col in seasonal_cols:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(f'An√°lisis de Serie Temporal ‚Äì {col}', fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # Graficar algunas parcelas para no saturar\n",
    "    for id_parcela in df_mstl['ID_PARCELA'].unique()[:10]:\n",
    "        ts = (\n",
    "            df_mstl[df_mstl['ID_PARCELA'] == id_parcela]\n",
    "            .groupby('fecha')[col]\n",
    "            .mean()\n",
    "            .dropna()\n",
    "            .sort_index()\n",
    "        )\n",
    "        \n",
    "        if len(ts) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Subplot 1: Serie temporal\n",
    "        axes[0].plot(ts.index, ts.values, linewidth=1.5, alpha=0.7, label=f'ID {id_parcela}')\n",
    "        \n",
    "        # Subplot 2: ACF\n",
    "        acf_values = np.correlate(ts.values - ts.values.mean(),\n",
    "                                 ts.values - ts.values.mean(), mode='full')\n",
    "        acf_values = acf_values[len(acf_values)//2:]\n",
    "        acf_values = acf_values / acf_values[0]\n",
    "        lags = range(min(100, len(ts)//2))\n",
    "        axes[1].plot(lags, acf_values[:len(lags)], alpha=0.7, linewidth=1.5,\n",
    "                    markersize=3, label=f'ID {id_parcela}')\n",
    "        \n",
    "        # Subplot 3: Periodograma\n",
    "        freqs, power = welch(ts.values, fs=fs, detrend='constant')\n",
    "        mask = freqs > 0\n",
    "        periods = 1.0 / freqs[mask]\n",
    "        power = power[mask]\n",
    "        axes[2].plot(periods, power, linewidth=1.5, alpha=0.7,\n",
    "                    markersize=4, label=f'ID {id_parcela}')\n",
    "\n",
    "    # Configurar ejes y leyendas\n",
    "    axes[0].set_title('Serie Temporal', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(ncol=2, fontsize=8)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_title('Funci√≥n de Autocorrelaci√≥n (ACF)', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Lag (semanas)', fontsize=11)\n",
    "    axes[1].set_ylabel('Autocorrelaci√≥n', fontsize=11)\n",
    "    axes[1].axvline(52, color='red', linestyle='--', alpha=0.5, linewidth=1.5,\n",
    "                   label='Lag anual (52 sem)')\n",
    "    axes[1].axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "    axes[1].legend(ncol=2, fontsize=8, loc='best')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_title('Periodograma', fontsize=13, fontweight='bold')\n",
    "    axes[2].axvline(52, color='red', linestyle='--', alpha=0.6)\n",
    "    axes[2].set_xlim(0, 300)\n",
    "    axes[2].legend(ncol=2, fontsize=8)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YORESA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
