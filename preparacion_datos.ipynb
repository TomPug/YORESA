{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caccfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ecf64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  COMUNIDAD AUT√ìNOMA  PROVINCIA  ESTADILLO CLASE SUBCLASE   PARCELA  \\\n",
      "0           La Rioja         26          1     A        1  260001A1   \n",
      "1           La Rioja         26          3     A        1  260003A1   \n",
      "2           La Rioja         26          4     A        1  260004A1   \n",
      "3           La Rioja         26          5     A        1  260005A1   \n",
      "4           La Rioja         26          6     A        4  260006A4   \n",
      "\n",
      "   INVENTARIO  HUSO  XED50_H30  YED50_H30    XETRS89     YETRS89  XETRS89_H30  \\\n",
      "0           4    30        NaN        NaN  500888.13  4720813.21    500888.13   \n",
      "1           4    30        NaN        NaN  496927.07  4719787.25    496927.07   \n",
      "2           4    30        NaN        NaN  502941.13  4719823.14    502941.13   \n",
      "3           4    30        NaN        NaN  503938.13  4719836.13    503938.13   \n",
      "4           4    30        NaN        NaN  510893.20  4719792.10    510893.20   \n",
      "\n",
      "   YETRS89_H30  XWGS84  YWGS84  TIPO DE COORDENADA  \n",
      "0   4720813.21     NaN     NaN                 NaN  \n",
      "1   4719787.25     NaN     NaN                 NaN  \n",
      "2   4719823.14     NaN     NaN                 NaN  \n",
      "3   4719836.13     NaN     NaN                 NaN  \n",
      "4   4719792.10     NaN     NaN                 NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\tpugn\\Downloads\\Coord_IFN_LaRioja.xlsx\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44ad45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa de puntos creada exitosamente\n",
      "N√∫mero de puntos: 1311\n",
      "CRS: EPSG:25830\n",
      "\n",
      "Primeros 5 puntos:\n",
      "  COMUNIDAD AUT√ìNOMA  PROVINCIA  ESTADILLO CLASE SUBCLASE   PARCELA  \\\n",
      "0           La Rioja         26          1     A        1  260001A1   \n",
      "1           La Rioja         26          3     A        1  260003A1   \n",
      "2           La Rioja         26          4     A        1  260004A1   \n",
      "3           La Rioja         26          5     A        1  260005A1   \n",
      "4           La Rioja         26          6     A        4  260006A4   \n",
      "\n",
      "   INVENTARIO  HUSO  XED50_H30  YED50_H30    XETRS89     YETRS89  XETRS89_H30  \\\n",
      "0           4    30        NaN        NaN  500888.13  4720813.21    500888.13   \n",
      "1           4    30        NaN        NaN  496927.07  4719787.25    496927.07   \n",
      "2           4    30        NaN        NaN  502941.13  4719823.14    502941.13   \n",
      "3           4    30        NaN        NaN  503938.13  4719836.13    503938.13   \n",
      "4           4    30        NaN        NaN  510893.20  4719792.10    510893.20   \n",
      "\n",
      "   YETRS89_H30  XWGS84  YWGS84  TIPO DE COORDENADA  \\\n",
      "0   4720813.21     NaN     NaN                 NaN   \n",
      "1   4719787.25     NaN     NaN                 NaN   \n",
      "2   4719823.14     NaN     NaN                 NaN   \n",
      "3   4719836.13     NaN     NaN                 NaN   \n",
      "4   4719792.10     NaN     NaN                 NaN   \n",
      "\n",
      "                       geometry  \n",
      "0  POINT (500888.13 4720813.21)  \n",
      "1  POINT (496927.07 4719787.25)  \n",
      "2  POINT (502941.13 4719823.14)  \n",
      "3  POINT (503938.13 4719836.13)  \n",
      "4    POINT (510893.2 4719792.1)  \n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear capa de puntos a partir de las coordenadas ETRS89\n",
    "# Crear geometr√≠a de puntos\n",
    "geometry = [Point(xy) for xy in zip(df['XETRS89_H30'], df['YETRS89_H30'])]\n",
    "\n",
    "# Crear GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:25830')\n",
    "\n",
    "print(\"Capa de puntos creada exitosamente\")\n",
    "print(f\"N√∫mero de puntos: {len(gdf)}\")\n",
    "print(f\"CRS: {gdf.crs}\")\n",
    "print(f\"\\nPrimeros 5 puntos:\")\n",
    "print(gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4961d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(r\"E:\\SAR_UVa\\shps\\puntos_ifn_rioja.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4b6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  ID_PARCELA       fecha  coherencia       band  SP_PPAL\n",
      "0           1          16  2017-01-03    0.900885  Coherence        1\n",
      "1           2          16  2017-01-09    0.952682  Coherence        1\n",
      "2           3          16  2017-01-15    0.970190  Coherence        1\n",
      "3           4          16  2017-01-21    0.833626  Coherence        1\n",
      "4           5          16  2017-01-27    0.709912  Coherence        1\n"
     ]
    }
   ],
   "source": [
    "df_coh = pd.read_csv(r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_modificado.csv\")\n",
    "print(df_coh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3d934c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      2\u001b[39m plt.hist(df_coh[\u001b[33m'\u001b[39m\u001b[33mcoherencia\u001b[39m\u001b[33m'\u001b[39m], bins=\u001b[32m50\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m, alpha=\u001b[32m0.7\u001b[39m)\n\u001b[32m      3\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mDistribuci√≥n de Valores de Coherencia\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_coh['coherencia'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribuci√≥n de Valores de Coherencia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dec3c8",
   "metadata": {},
   "source": [
    "## Codigo de calculo de parametros estadisticos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d62561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISIS DE SERIES TEMPORALES CON AUC\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Datos cargados desde: E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_modificado\n",
      "   Registros: 337500\n",
      "   Columnas: ['Unnamed: 0', 'ID_PARCELA', 'fecha', 'coherencia', 'band', 'SP_PPAL']\n",
      "   Bandas encontradas: <StringArray>\n",
      "['Coherence']\n",
      "Length: 1, dtype: str\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tpugn\\Documents\\YORESA\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    145\u001b[39m     row_data[\u001b[33m'\u001b[39m\u001b[33mstd_yearly_value\u001b[39m\u001b[33m'\u001b[39m] = np.nan\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# ========== 1. MEDIA Y DESVIACI√ìN EST√ÅNDAR POR ESTACI√ìN ==========\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m data_fid_band[\u001b[33m'\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata_fid_band\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.dt.quarter.map({\n\u001b[32m    149\u001b[39m     \u001b[32m1\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWinter\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m2\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSpring\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSummer\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m4\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mFall\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    150\u001b[39m })\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m season \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mWinter\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSpring\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSummer\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFall\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    153\u001b[39m     season_data = data_fid_band[data_fid_band[\u001b[33m'\u001b[39m\u001b[33mseason\u001b[39m\u001b[33m'\u001b[39m] == season][\u001b[33m'\u001b[39m\u001b[33mcoherencia\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tpugn\\Documents\\YORESA\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tpugn\\Documents\\YORESA\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'date'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script para calcular estad√≠sticas de series temporales con AUC del a√±o medio\n",
    "Incluye:\n",
    "- √Årea bajo la curva (AUC) del a√±o medio por banda\n",
    "- Media y desviaci√≥n est√°ndar por estaci√≥n\n",
    "- ACF en lags espec√≠ficos\n",
    "- Q-test (Ljung-Box)\n",
    "- √çndices del periodograma\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy import integrate\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "# Ruta del archivo CSV de entrada\n",
    "input_file = r\"E:\\SAR_UVa\\CSV'S\\valores_coherencia_extremadura_modificado\"\n",
    "output_file = 'estadisticas_series_temporales_.csv'\n",
    "\n",
    "# Par√°metros de an√°lisis\n",
    "fs = 1.0      # 1 observaci√≥n por semana\n",
    "nperseg = 52  # ventana anual\n",
    "freq_muestreo = 52  # frecuencia de muestreo anual (semanas)\n",
    "\n",
    "# Lags para ACF\n",
    "acf_lags = [26, 52, 78, 104, 156]\n",
    "\n",
    "# Lags para Q-test (Ljung-Box)\n",
    "qtest_lags = [1, 2, 3, 26, 52, 104, 156]\n",
    "\n",
    "# ============================================================================\n",
    "# CARGA DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE SERIES TEMPORALES CON AUC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not Path(input_file).exists():\n",
    "    print(f\"‚ùå Error: El archivo {input_file} no existe\")\n",
    "    exit(1)\n",
    "\n",
    "band_weekly = pd.read_csv(input_file)\n",
    "band_weekly['fecha'] = pd.to_datetime(band_weekly['fecha'])\n",
    "\n",
    "print(f\"\\n‚úÖ Datos cargados desde: {input_file}\")\n",
    "print(f\"   Registros: {len(band_weekly)}\")\n",
    "print(f\"   Columnas: {band_weekly.columns.tolist()}\")\n",
    "\n",
    "# Obtener bandas disponibles\n",
    "bands = band_weekly['band'].unique()\n",
    "print(f\"   Bandas encontradas: {bands}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "results = []\n",
    "total_parcelas = len(band_weekly['ID_PARCELA'].unique())\n",
    "contador = 0\n",
    "\n",
    "for fid in band_weekly['ID_PARCELA'].unique():\n",
    "    contador += 1\n",
    "    if contador % 50 == 0:\n",
    "        print(f\"Procesando parcela {contador}/{total_parcelas}...\")\n",
    "    \n",
    "    # Obtener la especie principal para este FID\n",
    "    sp_ppal = band_weekly[band_weekly['ID_PARCELA'] == fid]['SP_PPAL'].iloc[0]\n",
    "    \n",
    "    # Iterar por cada banda\n",
    "    for band in bands:\n",
    "        \n",
    "        # Filtrar datos por FID y banda\n",
    "        data_fid_band = band_weekly[\n",
    "            (band_weekly['ID_PARCELA'] == fid) &\n",
    "            (band_weekly['band'] == band)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(data_fid_band) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Serie temporal completa\n",
    "        ts = (\n",
    "            data_fid_band\n",
    "            .groupby('fecha')['coherencia']\n",
    "            .mean()\n",
    "            .dropna()\n",
    "            .sort_index()\n",
    "        )\n",
    "        \n",
    "        if len(ts) < nperseg:\n",
    "            continue\n",
    "        \n",
    "        # Diccionario para almacenar resultados de este FID-banda\n",
    "        row_data = {\n",
    "            'ID_PARCELA': fid,\n",
    "            'SP_PPAL': sp_ppal,\n",
    "            'band': band\n",
    "        }\n",
    "        \n",
    "        # ========== 0. √ÅREA BAJO LA CURVA DEL A√ëO MEDIO POR BANDA ==========\n",
    "        # Obtener el a√±o medio (agregaci√≥n semanal por semana del a√±o)\n",
    "        data_fid_band['week_of_year'] = data_fid_band['fecha'].dt.isocalendar().week\n",
    "        \n",
    "        # Agrupar por semana del a√±o y calcular la media\n",
    "        yearly_mean = (\n",
    "            data_fid_band\n",
    "            .groupby('week_of_year')['coherencia']\n",
    "            .mean()\n",
    "            .sort_index()\n",
    "        )\n",
    "        \n",
    "        if len(yearly_mean) > 0:\n",
    "            # Calcular AUC usando integraci√≥n trapezoidal\n",
    "            x = np.arange(len(yearly_mean))  # semanas 0-51\n",
    "            y = yearly_mean.values\n",
    "            \n",
    "            # AUC con m√©todo trapezoidal\n",
    "            auc = integrate.trapezoid(y, x)\n",
    "            row_data['auc_yearly_mean'] = auc\n",
    "            \n",
    "            # AUC normalizado (dividido por n√∫mero de semanas)\n",
    "            row_data['auc_yearly_mean_normalized'] = auc / len(yearly_mean)\n",
    "            \n",
    "            # Valor promedio del a√±o (alternativa simple)\n",
    "            row_data['mean_yearly_value'] = np.mean(y)\n",
    "            \n",
    "            # M√≠nimo y m√°ximo del a√±o medio\n",
    "            row_data['min_yearly_value'] = np.min(y)\n",
    "            row_data['max_yearly_value'] = np.max(y)\n",
    "            row_data['std_yearly_value'] = np.std(y)\n",
    "        else:\n",
    "            row_data['auc_yearly_mean'] = np.nan\n",
    "            row_data['auc_yearly_mean_normalized'] = np.nan\n",
    "            row_data['mean_yearly_value'] = np.nan\n",
    "            row_data['min_yearly_value'] = np.nan\n",
    "            row_data['max_yearly_value'] = np.nan\n",
    "            row_data['std_yearly_value'] = np.nan\n",
    "        \n",
    "        # ========== 1. MEDIA Y DESVIACI√ìN EST√ÅNDAR POR ESTACI√ìN ==========\n",
    "        data_fid_band['season'] = data_fid_band['date'].dt.quarter.map({\n",
    "            1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'\n",
    "        })\n",
    "        \n",
    "        for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
    "            season_data = data_fid_band[data_fid_band['season'] == season]['coherencia']\n",
    "            row_data[f'mean_{season}'] = season_data.mean() if len(season_data) > 0 else np.nan\n",
    "            row_data[f'std_{season}'] = season_data.std() if len(season_data) > 0 else np.nan\n",
    "        \n",
    "        # ========== 2. ACF EN LAGS ESPEC√çFICOS ==========\n",
    "        try:\n",
    "            # Calcular ACF\n",
    "            acf_values = acf(ts.values, nlags=max(acf_lags), fft=True)\n",
    "            \n",
    "            for lag in acf_lags:\n",
    "                if lag < len(acf_values):\n",
    "                    row_data[f'acf_lag_{lag}'] = acf_values[lag]\n",
    "                else:\n",
    "                    row_data[f'acf_lag_{lag}'] = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error ACF para ID_PARCELA={fid}, band={band}: {e}\")\n",
    "            for lag in acf_lags:\n",
    "                row_data[f'acf_lag_{lag}'] = np.nan\n",
    "        \n",
    "        # ========== 3. Q-TEST (LJUNG-BOX) EN LAGS ESPEC√çFICOS ==========\n",
    "        try:\n",
    "            # Calcular Ljung-Box test\n",
    "            lb_result = acorr_ljungbox(ts.values, lags=qtest_lags, return_df=True)\n",
    "            \n",
    "            for lag in qtest_lags:\n",
    "                if lag in lb_result.index:\n",
    "                    row_data[f'qtest_stat_{lag}'] = lb_result.loc[lag, 'lb_stat']\n",
    "                    row_data[f'qtest_pvalue_{lag}'] = lb_result.loc[lag, 'lb_pvalue']\n",
    "                else:\n",
    "                    row_data[f'qtest_stat_{lag}'] = np.nan\n",
    "                    row_data[f'qtest_pvalue_{lag}'] = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error Q-test para ID_PARCELA={fid}, band={band}: {e}\")\n",
    "            for lag in qtest_lags:\n",
    "                row_data[f'qtest_stat_{lag}'] = np.nan\n",
    "                row_data[f'qtest_pvalue_{lag}'] = np.nan\n",
    "        \n",
    "        # ========== 4. √çNDICES DEL PERIODOGRAMA ==========\n",
    "        try:\n",
    "            # Calcular periodograma con Welch\n",
    "            freqs, power = welch(\n",
    "                ts.values, \n",
    "                fs=fs, \n",
    "                detrend='constant',\n",
    "                scaling='density'\n",
    "            )\n",
    "            \n",
    "            # Convertir frecuencias a periodos\n",
    "            mask = freqs > 0\n",
    "            periodos = 1.0 / freqs[mask]\n",
    "            power = power[mask]\n",
    "            \n",
    "            # Ciclos de inter√©s\n",
    "            ciclos = np.array([freq_muestreo/3, freq_muestreo/2, freq_muestreo])\n",
    "            \n",
    "            # Encontrar posiciones de los ciclos\n",
    "            pos_ciclos = [np.argmin(np.abs(periodos - ciclo)) for ciclo in ciclos]\n",
    "            \n",
    "            # Extraer bandas de los ciclos\n",
    "            bands_power = power[pos_ciclos]\n",
    "            \n",
    "            # 4.1 Seasonality Mode (periodo con m√°xima potencia)\n",
    "            max_idx = np.argmax(bands_power)\n",
    "            row_data['seasonality_mode'] = ciclos[max_idx]\n",
    "            \n",
    "            # 4.2 Fisher Kappa (max / mean)\n",
    "            max_power = np.max(bands_power)\n",
    "            mean_power = np.mean(bands_power)\n",
    "            row_data['fisher_kappa'] = max_power / mean_power if mean_power > 0 else np.nan\n",
    "            \n",
    "            # 4.3 Seasonality Stability\n",
    "            power_ciclo_anual_adelante = np.sum(power[pos_ciclos[2]:])\n",
    "            row_data['seasonality_stability'] = (\n",
    "                max_power / power_ciclo_anual_adelante \n",
    "                if power_ciclo_anual_adelante > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # 4.4 Plurianual Cycles\n",
    "            power_plurianual = np.sum(power[:pos_ciclos[2]])\n",
    "            power_total = np.sum(power)\n",
    "            row_data['plurianual_cycles'] = (\n",
    "                power_plurianual / power_total \n",
    "                if power_total > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # 4.5 Seasonality Amplitude\n",
    "            row_data['seasonality_amplitude'] = max_power\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error Periodograma para fid={fid}, band={band}: {e}\")\n",
    "            row_data['seasonality_mode'] = np.nan\n",
    "            row_data['fisher_kappa'] = np.nan\n",
    "            row_data['seasonality_stability'] = np.nan\n",
    "            row_data['plurianual_cycles'] = np.nan\n",
    "            row_data['seasonality_amplitude'] = np.nan\n",
    "        \n",
    "        # Agregar fila a resultados\n",
    "        results.append(row_data)\n",
    "\n",
    "# ============================================================================\n",
    "# GUARDAR RESULTADOS\n",
    "# ============================================================================\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Reordenar columnas\n",
    "cols = ['ID_PARCELA', 'SP_PPAL', 'band'] + [col for col in df_results.columns if col not in ['ID_PARCELA', 'SP_PPAL', 'band']]\n",
    "df_results = df_results[cols]\n",
    "\n",
    "# Guardar CSV\n",
    "df_results.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ AN√ÅLISIS COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Archivo guardado en: {output_file}\")\n",
    "print(f\"Total de registros: {len(df_results)}\")\n",
    "print(f\"\\nColumnas generadas ({len(df_results.columns)}):\")\n",
    "for i, col in enumerate(df_results.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nüìä Resumen de AUC por banda:\")\n",
    "auc_summary = df_results.groupby('band')[['auc_yearly_mean', 'auc_yearly_mean_normalized', 'mean_yearly_value']].describe()\n",
    "print(auc_summary)\n",
    "\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "print(df_results.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YORESA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
